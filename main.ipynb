{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T08:27:21.080538Z",
     "start_time": "2025-03-11T08:27:20.935226Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import cmath\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our buildDataSet function here!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T08:27:21.986652Z",
     "start_time": "2025-03-11T08:27:21.978251Z"
    }
   },
   "source": [
    "def buildDataSet(max_amplitude,min_sparsity,max_sparsity,vector_size,data_set_size):\n",
    "    sparse_data = np.zeros((vector_size,data_set_size)) # Initialize the sparse_data matrix\n",
    "\n",
    "    # Iterate over the columns of the sparse_data matrix to define the data samples\n",
    "    for i in range(data_set_size):\n",
    "        sparsity = random.randint(min_sparsity,max_sparsity)\n",
    "        indices = random.sample(range(vector_size),sparsity)\n",
    "        amps = random.sample(range(max_amplitude),sparsity)\n",
    "        sparse_data[indices,i] = amps\n",
    "    \n",
    "    # Define the DFT matrix and multiply our spare_data vectors with it to find dense data\n",
    "    DFT = sp.linalg.dft(vector_size)/np.sqrt(vector_size)\n",
    "    dense_data = DFT@sparse_data\n",
    "    return dense_data,sparse_data"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:52.883067Z",
     "start_time": "2025-03-11T09:04:52.739694Z"
    }
   },
   "source": [
    "max_amplitude = 10\n",
    "min_sparsity = 3\n",
    "max_sparsity = 5\n",
    "vector_size = 4*100\n",
    "data_set_size = 1000\n",
    "dense_data, sparse_data = buildDataSet(max_amplitude,min_sparsity,max_sparsity,vector_size,data_set_size)    "
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T08:27:38.076044Z",
     "start_time": "2025-03-11T08:27:38.012400Z"
    }
   },
   "source": [
    "DFT = sp.linalg.dft(vector_size)/np.sqrt(vector_size)\n",
    "iDFT = DFT.conj().T\n",
    "\n",
    "iDFT@dense_data"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.32383925e-17-5.30495957e-15j -3.75908315e-15+1.55827220e-15j\n",
      "  -5.67563560e-15-1.27511087e-14j ... -3.91905967e-16-5.59725313e-15j\n",
      "   1.04465676e-14-1.07786490e-14j -4.08380551e-15+1.53525282e-14j]\n",
      " [-4.32645966e-15-2.81770807e-15j -5.86865948e-15-3.66864353e-15j\n",
      "  -3.03794343e-15+1.18800780e-14j ... -5.78088144e-15-1.58742081e-14j\n",
      "  -5.20309979e-15-1.46985806e-14j  1.69142069e-14-2.48480443e-14j]\n",
      " [-3.93647314e-15+2.13124858e-15j  1.54628366e-15+2.77167802e-15j\n",
      "   1.08119353e-15-1.26130583e-15j ... -2.32706105e-14+1.29636896e-14j\n",
      "  -1.14119808e-14-1.77664643e-15j -1.68235505e-14-4.48146349e-15j]\n",
      " ...\n",
      " [-2.57483018e-15-2.00013029e-15j -1.01230986e-14+4.56469969e-15j\n",
      "  -3.94736216e-15+1.61422553e-14j ...  9.22216633e-15+1.74251633e-14j\n",
      "  -1.91337725e-14-1.24438804e-14j  3.18320655e-15-1.08903105e-14j]\n",
      " [-1.11613546e-14-7.53323639e-16j -7.43453566e-15+6.46876808e-15j\n",
      "   3.93988894e-15-7.05815539e-17j ... -4.74455272e-15+2.32939712e-15j\n",
      "  -1.59863324e-14+1.48105404e-14j -2.92970740e-15+4.46371822e-16j]\n",
      " [-1.91944456e-15+9.57976889e-15j -4.45329689e-17+7.14927301e-15j\n",
      "   3.25575713e-15+1.55569707e-14j ...  2.74191166e-15+9.35953689e-15j\n",
      "  -8.30866660e-15+1.23301986e-14j  6.25809785e-15-5.39376326e-15j]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that our vectors are very sparse if we take the IDFT\n",
    "\n",
    "## Setting up the dataset for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:06.020302Z",
     "start_time": "2025-03-11T09:04:06.008358Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.concatenate((dense_data.real, dense_data.imag), axis=0).T\n",
    "Y = np.concatenate((sparse_data.real, sparse_data.imag), axis=0).T\n",
    "\n",
    "X_tensor = torch.tensor(X,dtype=torch.float)\n",
    "Y_tensor = torch.tensor(Y,dtype=torch.float)\n",
    "dataset = TensorDataset(X_tensor,Y_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size = 200,shuffle = True)"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the neural network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:54.856135Z",
     "start_time": "2025-03-11T09:04:54.836370Z"
    }
   },
   "source": [
    "# class ExponentialComplexLinear(nn.Module):\n",
    "#     def __init__(self, in_features, out_features):\n",
    "#         super(ExponentialComplexLinear, self).__init__()\n",
    "#\n",
    "#         # Trainable phase angles (real-valued), this is where we are training the q-values in the context of the exercise\n",
    "#         self.phases = nn.Parameter(torch.randn(out_features, in_features) * 0.1)  # Small random initialization\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # Compute our matrix\n",
    "#         W = torch.exp(1j * self.phases)  # Enforces |W| = 1\n",
    "#\n",
    "#         # Complex matrix multiplication\n",
    "#         return torch.matmul(x, W.t())\n",
    "\n",
    "class ComplexDecoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, output_dim):\n",
    "        super(ComplexDecoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(encoding_dim,400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.complex(F.relu(x.real), F.relu(x.imag))  # Apply ReLU separately\n",
    "\n",
    "# class LearnedAutoencoder(nn.Module):\n",
    "#     def __init__(self, input_dim, encoding_dim):\n",
    "#         super(LearnedAutoencoder, self).__init__()\n",
    "#         # Encoder: Maps from input_dim to a lower-dimensional complex encoding space.\n",
    "#         self.encoder = ExponentialComplexLinear(input_dim, encoding_dim)\n",
    "#\n",
    "#         # Decoder: Maps from the encoding space back to the original input dimension.\n",
    "#         self.decoder = ComplexDecoder(encoding_dim, input_dim)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         encoded = self.encoder(x)\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         return decoded\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:57.710570Z",
     "start_time": "2025-03-11T09:04:57.636024Z"
    }
   },
   "source": [
    "# Define the size of our \"measurement\" vector as encoding_dim. This needs to be larger than the sparsity of our matrix\n",
    "\n",
    "encoding_dim = max_sparsity + 1\n",
    "\n",
    "# Initialize model\n",
    "model = ComplexDecoder(vector_size, encoding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def complex_mse_loss(input, target):\n",
    "    return F.mse_loss(input.real, target.real) + F.mse_loss(input.imag, target.imag)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch  # Unpack the tuple\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = complex_mse_loss(output, targets)  # Custom loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4873/636337709.py:11: UserWarning: Using a target size (torch.Size([200, 400])) that is different to the input size (torch.Size([200, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input.real, target.real) + F.mse_loss(input.imag, target.imag)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (400) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[87]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     17\u001B[39m optimizer.zero_grad()\n\u001B[32m     18\u001B[39m output = model(inputs)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m loss = \u001B[43mcomplex_mse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Custom loss\u001B[39;00m\n\u001B[32m     20\u001B[39m loss.backward()\n\u001B[32m     21\u001B[39m optimizer.step()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[87]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mcomplex_mse_loss\u001B[39m\u001B[34m(input, target)\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcomplex_mse_loss\u001B[39m(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreal\u001B[49m\u001B[43m)\u001B[49m + F.mse_loss(\u001B[38;5;28minput\u001B[39m.imag, target.imag)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Compression/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3884\u001B[39m, in \u001B[36mmse_loss\u001B[39m\u001B[34m(input, target, size_average, reduce, reduction, weight)\u001B[39m\n\u001B[32m   3881\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3882\u001B[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001B[32m-> \u001B[39m\u001B[32m3884\u001B[39m expanded_input, expanded_target = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3886\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3887\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m weight.size() != \u001B[38;5;28minput\u001B[39m.size():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Compression/.venv/lib/python3.12/site-packages/torch/functional.py:76\u001B[39m, in \u001B[36mbroadcast_tensors\u001B[39m\u001B[34m(*tensors)\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (6) must match the size of tensor b (400) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 87
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
