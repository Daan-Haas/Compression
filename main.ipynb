{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import cmath\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our buildDataSet function here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(max_amplitude,min_sparsity,max_sparsity,vector_size,data_set_size):\n",
    "    sparse_data = np.zeros((vector_size,data_set_size)) # Initialize the sparse_data matrix\n",
    "\n",
    "    # Iterate over the columns of the sparse_data matrix to define the data samples\n",
    "    for i in range(data_set_size):\n",
    "        sparsity = random.randint(min_sparsity,max_sparsity)\n",
    "        indices = random.sample(range(vector_size),sparsity)\n",
    "        amps = random.sample(range(max_amplitude),sparsity)\n",
    "        sparse_data[indices,i] = amps\n",
    "    \n",
    "    # Define the DFT matrix and multiply our spare_data vectors with it to find dense data\n",
    "    DFT = sp.linalg.dft(vector_size)/np.sqrt(vector_size)\n",
    "    dense_data = DFT@sparse_data\n",
    "    return dense_data,sparse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_amplitude = 10\n",
    "min_sparsity = 3\n",
    "max_sparsity = 5\n",
    "vector_size = 100\n",
    "data_set_size = 1000\n",
    "dense_data, sparse_data = buildDataSet(max_amplitude,min_sparsity,max_sparsity,vector_size,data_set_size)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.99610345e-16-1.84663568e-14j,  4.18125245e-15+2.93396784e-14j,\n",
       "        -1.45951537e-15+2.79218236e-15j, ...,\n",
       "         1.06855842e-14+6.18150753e-15j,  1.18998974e-14-4.44133976e-15j,\n",
       "         5.66867945e-15+3.52969263e-15j],\n",
       "       [-8.41028613e-15-2.31454568e-14j,  1.98885988e-14-1.13984708e-14j,\n",
       "         2.83982701e-15+4.11576218e-16j, ...,\n",
       "         2.17603713e-14+5.35210759e-15j, -3.89004695e-17+2.76226612e-15j,\n",
       "        -3.67374563e-15-8.08479995e-15j],\n",
       "       [-8.68842742e-15+1.79566577e-14j,  7.35454883e-15-1.98773856e-14j,\n",
       "        -1.23309480e-15-1.64208581e-16j, ...,\n",
       "         1.08437649e-15-1.42661373e-14j, -4.21884749e-15-2.73137715e-15j,\n",
       "         3.26839427e-15+7.26824185e-15j],\n",
       "       ...,\n",
       "       [ 9.00000000e+00-2.13786998e-15j, -7.31413323e-15+1.32191195e-14j,\n",
       "         3.33252339e-15+6.34271632e-15j, ...,\n",
       "         1.21609612e-15+3.71609140e-15j, -6.12690643e-15-9.99436479e-15j,\n",
       "        -3.71469862e-15-1.22542494e-14j],\n",
       "       [-3.67351837e-15-1.82573956e-14j,  2.15063422e-15-2.33089674e-16j,\n",
       "         8.65022198e-15+5.94837691e-15j, ...,\n",
       "         5.70615954e-15+4.88858015e-15j,  2.00000000e+00-8.44235587e-15j,\n",
       "        -1.87932707e-15-2.44000366e-15j],\n",
       "       [ 4.65933598e-15+6.08139943e-15j,  3.05580261e-15+5.34536397e-15j,\n",
       "        -7.70627578e-16-1.84750781e-14j, ...,\n",
       "         4.00000000e+00-4.84133613e-15j,  2.04556742e-15+2.31874863e-14j,\n",
       "        -8.75586624e-16-4.69646206e-15j]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFT = sp.linalg.dft(vector_size)/np.sqrt(vector_size)\n",
    "iDFT = DFT.conj().T\n",
    "\n",
    "iDFT@dense_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that our vectors are very sparse if we take the IDFT\n",
    "\n",
    "## Setting up the dataset for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dense_data.T\n",
    "\n",
    "X_tensor = torch.tensor(X,dtype=torch.cfloat)\n",
    "\n",
    "dataset = TensorDataset(X_tensor,X_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size = 100,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialComplexLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ExponentialComplexLinear, self).__init__()\n",
    "\n",
    "        # Trainable phase angles (real-valued), this is where we are training the q-values in the context of the exercise\n",
    "        self.phases = nn.Parameter(torch.randn(out_features, in_features) * 0.1)  # Small random initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute our matrix\n",
    "        W = torch.exp(1j * self.phases)  # Enforces |W| = 1\n",
    "\n",
    "        # Complex matrix multiplication\n",
    "        return torch.matmul(x, W.t())\n",
    "\n",
    "class ComplexDecoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, output_dim):\n",
    "        super(ComplexDecoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            ExponentialComplexLinear(encoding_dim, 128),  # Complex Linear Layer\n",
    "            ComplexReLU(),  # Activation\n",
    "            ExponentialComplexLinear(128, 256),\n",
    "            ComplexReLU(),\n",
    "            ExponentialComplexLinear(256, output_dim)  # Final output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.complex(F.relu(x.real), F.relu(x.imag))  # Apply ReLU separately\n",
    "\n",
    "class LearnedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(LearnedAutoencoder, self).__init__()\n",
    "        # Encoder: Maps from input_dim to a lower-dimensional complex encoding space.\n",
    "        self.encoder = ExponentialComplexLinear(input_dim, encoding_dim)\n",
    "        \n",
    "        # Decoder: Maps from the encoding space back to the original input dimension.\n",
    "        self.decoder = ComplexDecoder(encoding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4205088079872.000000\n",
      "Epoch 2, Loss: 4305087488.000000\n",
      "Epoch 3, Loss: 3548248539136.000000\n",
      "Epoch 4, Loss: 5219619962880.000000\n",
      "Epoch 5, Loss: 3042452701184.000000\n",
      "Epoch 6, Loss: 2623023874048.000000\n",
      "Epoch 7, Loss: 5904337469440.000000\n",
      "Epoch 8, Loss: 2220238569472.000000\n",
      "Epoch 9, Loss: 4720451125248.000000\n",
      "Epoch 10, Loss: 4465557504000.000000\n",
      "Epoch 11, Loss: 40096727040.000000\n",
      "Epoch 12, Loss: 2155561222144.000000\n",
      "Epoch 13, Loss: 1558163619840.000000\n",
      "Epoch 14, Loss: 2909429301248.000000\n",
      "Epoch 15, Loss: 2737407000576.000000\n",
      "Epoch 16, Loss: 6558643126272.000000\n",
      "Epoch 17, Loss: 2890331324416.000000\n",
      "Epoch 18, Loss: 4591495151616.000000\n",
      "Epoch 19, Loss: 5949719314432.000000\n",
      "Epoch 20, Loss: 2520901484544.000000\n",
      "Epoch 21, Loss: 2365512744960.000000\n",
      "Epoch 22, Loss: 1380905385984.000000\n",
      "Epoch 23, Loss: 2240828932096.000000\n",
      "Epoch 24, Loss: 3343725625344.000000\n",
      "Epoch 25, Loss: 2209521074176.000000\n",
      "Epoch 26, Loss: 948561903616.000000\n",
      "Epoch 27, Loss: 5741143916544.000000\n",
      "Epoch 28, Loss: 4806813417472.000000\n",
      "Epoch 29, Loss: 597639495680.000000\n",
      "Epoch 30, Loss: 55336488960.000000\n",
      "Epoch 31, Loss: 1849834078208.000000\n",
      "Epoch 32, Loss: 3647797198848.000000\n",
      "Epoch 33, Loss: 3706617856000.000000\n",
      "Epoch 34, Loss: 2693364973568.000000\n",
      "Epoch 35, Loss: 10180401152.000000\n",
      "Epoch 36, Loss: 3380693434368.000000\n",
      "Epoch 37, Loss: 2911887425536.000000\n",
      "Epoch 38, Loss: 691370590208.000000\n",
      "Epoch 39, Loss: 700357935104.000000\n",
      "Epoch 40, Loss: 859643117568.000000\n",
      "Epoch 41, Loss: 43480784896.000000\n",
      "Epoch 42, Loss: 1237715386368.000000\n",
      "Epoch 43, Loss: 1693697179648.000000\n",
      "Epoch 44, Loss: 970386767872.000000\n",
      "Epoch 45, Loss: 855656103936.000000\n",
      "Epoch 46, Loss: 2682563592192.000000\n",
      "Epoch 47, Loss: 4146040143872.000000\n",
      "Epoch 48, Loss: 1695441485824.000000\n",
      "Epoch 49, Loss: 991288098816.000000\n",
      "Epoch 50, Loss: 1172018298880.000000\n",
      "Epoch 51, Loss: 564136837120.000000\n",
      "Epoch 52, Loss: 900903534592.000000\n",
      "Epoch 53, Loss: 939299176448.000000\n",
      "Epoch 54, Loss: 2034369953792.000000\n",
      "Epoch 55, Loss: 508452470784.000000\n",
      "Epoch 56, Loss: 932282105856.000000\n",
      "Epoch 57, Loss: 885224701952.000000\n",
      "Epoch 58, Loss: 19477291008.000000\n",
      "Epoch 59, Loss: 1231108440064.000000\n",
      "Epoch 60, Loss: 1196735725568.000000\n",
      "Epoch 61, Loss: 1660382871552.000000\n",
      "Epoch 62, Loss: 771599368192.000000\n",
      "Epoch 63, Loss: 505247662080.000000\n",
      "Epoch 64, Loss: 648851619840.000000\n",
      "Epoch 65, Loss: 118688112640.000000\n",
      "Epoch 66, Loss: 901500502016.000000\n",
      "Epoch 67, Loss: 537175818240.000000\n",
      "Epoch 68, Loss: 1555816120320.000000\n",
      "Epoch 69, Loss: 545721548800.000000\n",
      "Epoch 70, Loss: 929747632128.000000\n",
      "Epoch 71, Loss: 665672613888.000000\n",
      "Epoch 72, Loss: 264212873216.000000\n",
      "Epoch 73, Loss: 250404044800.000000\n",
      "Epoch 74, Loss: 719671853056.000000\n",
      "Epoch 75, Loss: 429246054400.000000\n",
      "Epoch 76, Loss: 720217178112.000000\n",
      "Epoch 77, Loss: 646923223040.000000\n",
      "Epoch 78, Loss: 8366693376.000000\n",
      "Epoch 79, Loss: 600181702656.000000\n",
      "Epoch 80, Loss: 216137678848.000000\n",
      "Epoch 81, Loss: 894839160832.000000\n",
      "Epoch 82, Loss: 687671738368.000000\n",
      "Epoch 83, Loss: 234523918336.000000\n",
      "Epoch 84, Loss: 361269755904.000000\n",
      "Epoch 85, Loss: 122669350912.000000\n",
      "Epoch 86, Loss: 465994055680.000000\n",
      "Epoch 87, Loss: 297972334592.000000\n",
      "Epoch 88, Loss: 127178457088.000000\n",
      "Epoch 89, Loss: 370054430720.000000\n",
      "Epoch 90, Loss: 326417678336.000000\n",
      "Epoch 91, Loss: 268586614784.000000\n",
      "Epoch 92, Loss: 4776086016.000000\n",
      "Epoch 93, Loss: 784986669056.000000\n",
      "Epoch 94, Loss: 132044742656.000000\n",
      "Epoch 95, Loss: 103558225920.000000\n",
      "Epoch 96, Loss: 539994324992.000000\n",
      "Epoch 97, Loss: 228289691648.000000\n",
      "Epoch 98, Loss: 187804319744.000000\n",
      "Epoch 99, Loss: 31253551104.000000\n",
      "Epoch 100, Loss: 385469317120.000000\n",
      "Epoch 101, Loss: 401082056704.000000\n",
      "Epoch 102, Loss: 472280989696.000000\n",
      "Epoch 103, Loss: 98332844032.000000\n",
      "Epoch 104, Loss: 36910178304.000000\n",
      "Epoch 105, Loss: 597532606464.000000\n",
      "Epoch 106, Loss: 611397271552.000000\n",
      "Epoch 107, Loss: 94293188608.000000\n",
      "Epoch 108, Loss: 104503640064.000000\n",
      "Epoch 109, Loss: 69578956800.000000\n",
      "Epoch 110, Loss: 191202115584.000000\n",
      "Epoch 111, Loss: 75899035648.000000\n",
      "Epoch 112, Loss: 234702192640.000000\n",
      "Epoch 113, Loss: 289535721472.000000\n",
      "Epoch 114, Loss: 294072811520.000000\n",
      "Epoch 115, Loss: 128838082560.000000\n",
      "Epoch 116, Loss: 302420688896.000000\n",
      "Epoch 117, Loss: 68480122880.000000\n",
      "Epoch 118, Loss: 247678795776.000000\n",
      "Epoch 119, Loss: 84780924928.000000\n",
      "Epoch 120, Loss: 344444698624.000000\n",
      "Epoch 121, Loss: 361708519424.000000\n",
      "Epoch 122, Loss: 101751603200.000000\n",
      "Epoch 123, Loss: 40014745600.000000\n",
      "Epoch 124, Loss: 123682963456.000000\n",
      "Epoch 125, Loss: 199186726912.000000\n",
      "Epoch 126, Loss: 227945938944.000000\n",
      "Epoch 127, Loss: 2195744512.000000\n",
      "Epoch 128, Loss: 46893432832.000000\n",
      "Epoch 129, Loss: 206960787456.000000\n",
      "Epoch 130, Loss: 233315581952.000000\n",
      "Epoch 131, Loss: 262886998016.000000\n",
      "Epoch 132, Loss: 136797331456.000000\n",
      "Epoch 133, Loss: 2581486336.000000\n",
      "Epoch 134, Loss: 81856356352.000000\n",
      "Epoch 135, Loss: 34108991488.000000\n",
      "Epoch 136, Loss: 81776992256.000000\n",
      "Epoch 137, Loss: 210553438208.000000\n",
      "Epoch 138, Loss: 84555210752.000000\n",
      "Epoch 139, Loss: 48682041344.000000\n",
      "Epoch 140, Loss: 11276664832.000000\n",
      "Epoch 141, Loss: 40213614592.000000\n",
      "Epoch 142, Loss: 60212703232.000000\n",
      "Epoch 143, Loss: 113906139136.000000\n",
      "Epoch 144, Loss: 279196663808.000000\n",
      "Epoch 145, Loss: 50680201216.000000\n",
      "Epoch 146, Loss: 32084705280.000000\n",
      "Epoch 147, Loss: 41049108480.000000\n",
      "Epoch 148, Loss: 42249261056.000000\n",
      "Epoch 149, Loss: 91742756864.000000\n",
      "Epoch 150, Loss: 26082299904.000000\n",
      "Epoch 151, Loss: 228553031680.000000\n",
      "Epoch 152, Loss: 53879791616.000000\n",
      "Epoch 153, Loss: 79872835584.000000\n",
      "Epoch 154, Loss: 52771426304.000000\n",
      "Epoch 155, Loss: 93733928960.000000\n",
      "Epoch 156, Loss: 134719832064.000000\n",
      "Epoch 157, Loss: 100945993728.000000\n",
      "Epoch 158, Loss: 28068184064.000000\n",
      "Epoch 159, Loss: 7260256256.000000\n",
      "Epoch 160, Loss: 35138158592.000000\n",
      "Epoch 161, Loss: 66888384512.000000\n",
      "Epoch 162, Loss: 123457093632.000000\n",
      "Epoch 163, Loss: 95283814400.000000\n",
      "Epoch 164, Loss: 25307025408.000000\n",
      "Epoch 165, Loss: 109273071616.000000\n",
      "Epoch 166, Loss: 43003744256.000000\n",
      "Epoch 167, Loss: 84964024320.000000\n",
      "Epoch 168, Loss: 128348962816.000000\n",
      "Epoch 169, Loss: 20007383040.000000\n",
      "Epoch 170, Loss: 59720765440.000000\n",
      "Epoch 171, Loss: 41569812480.000000\n",
      "Epoch 172, Loss: 70111961088.000000\n",
      "Epoch 173, Loss: 5815247360.000000\n",
      "Epoch 174, Loss: 54583586816.000000\n",
      "Epoch 175, Loss: 51001835520.000000\n",
      "Epoch 176, Loss: 49120305152.000000\n",
      "Epoch 177, Loss: 63712129024.000000\n",
      "Epoch 178, Loss: 28983652352.000000\n",
      "Epoch 179, Loss: 63170076672.000000\n",
      "Epoch 180, Loss: 38012596224.000000\n",
      "Epoch 181, Loss: 15582171136.000000\n",
      "Epoch 182, Loss: 68230721536.000000\n",
      "Epoch 183, Loss: 18185377792.000000\n",
      "Epoch 184, Loss: 62075363328.000000\n",
      "Epoch 185, Loss: 64923979776.000000\n",
      "Epoch 186, Loss: 46177058816.000000\n",
      "Epoch 187, Loss: 55062388736.000000\n",
      "Epoch 188, Loss: 65877323776.000000\n",
      "Epoch 189, Loss: 44687781888.000000\n",
      "Epoch 190, Loss: 18248316928.000000\n",
      "Epoch 191, Loss: 35208568832.000000\n",
      "Epoch 192, Loss: 37776572416.000000\n",
      "Epoch 193, Loss: 48406491136.000000\n",
      "Epoch 194, Loss: 14106341376.000000\n",
      "Epoch 195, Loss: 31161071616.000000\n",
      "Epoch 196, Loss: 25561192448.000000\n",
      "Epoch 197, Loss: 37472608256.000000\n",
      "Epoch 198, Loss: 42186915840.000000\n",
      "Epoch 199, Loss: 24480364544.000000\n",
      "Epoch 200, Loss: 38949662720.000000\n",
      "Epoch 201, Loss: 38122016768.000000\n",
      "Epoch 202, Loss: 5041917440.000000\n",
      "Epoch 203, Loss: 822590720.000000\n",
      "Epoch 204, Loss: 672111296.000000\n",
      "Epoch 205, Loss: 47635378176.000000\n",
      "Epoch 206, Loss: 56758452224.000000\n",
      "Epoch 207, Loss: 23377002496.000000\n",
      "Epoch 208, Loss: 1098582528.000000\n",
      "Epoch 209, Loss: 11296168960.000000\n",
      "Epoch 210, Loss: 10356841472.000000\n",
      "Epoch 211, Loss: 34469097472.000000\n",
      "Epoch 212, Loss: 12394645504.000000\n",
      "Epoch 213, Loss: 6160791552.000000\n",
      "Epoch 214, Loss: 40780001280.000000\n",
      "Epoch 215, Loss: 696605888.000000\n",
      "Epoch 216, Loss: 44264099840.000000\n",
      "Epoch 217, Loss: 22236440576.000000\n",
      "Epoch 218, Loss: 18588254208.000000\n",
      "Epoch 219, Loss: 8693767168.000000\n",
      "Epoch 220, Loss: 27768719360.000000\n",
      "Epoch 221, Loss: 42177138688.000000\n",
      "Epoch 222, Loss: 3300034560.000000\n",
      "Epoch 223, Loss: 22023507968.000000\n",
      "Epoch 224, Loss: 43854082048.000000\n",
      "Epoch 225, Loss: 12413413376.000000\n",
      "Epoch 226, Loss: 22810132480.000000\n",
      "Epoch 227, Loss: 4978383872.000000\n",
      "Epoch 228, Loss: 15016457216.000000\n",
      "Epoch 229, Loss: 1048750016.000000\n",
      "Epoch 230, Loss: 19062953984.000000\n",
      "Epoch 231, Loss: 20170983424.000000\n",
      "Epoch 232, Loss: 13403528192.000000\n",
      "Epoch 233, Loss: 20390064128.000000\n",
      "Epoch 234, Loss: 4944008704.000000\n",
      "Epoch 235, Loss: 25969776640.000000\n",
      "Epoch 236, Loss: 20531933184.000000\n",
      "Epoch 237, Loss: 13178243072.000000\n",
      "Epoch 238, Loss: 13156210688.000000\n",
      "Epoch 239, Loss: 460433792.000000\n",
      "Epoch 240, Loss: 682625920.000000\n",
      "Epoch 241, Loss: 19042846720.000000\n",
      "Epoch 242, Loss: 6234658304.000000\n",
      "Epoch 243, Loss: 12728036352.000000\n",
      "Epoch 244, Loss: 14813187072.000000\n",
      "Epoch 245, Loss: 575179328.000000\n",
      "Epoch 246, Loss: 692684800.000000\n",
      "Epoch 247, Loss: 15610253312.000000\n",
      "Epoch 248, Loss: 678440512.000000\n",
      "Epoch 249, Loss: 17428547584.000000\n",
      "Epoch 250, Loss: 8986278912.000000\n",
      "Epoch 251, Loss: 23266797568.000000\n",
      "Epoch 252, Loss: 539294848.000000\n",
      "Epoch 253, Loss: 5514762240.000000\n",
      "Epoch 254, Loss: 1809695104.000000\n",
      "Epoch 255, Loss: 11661891584.000000\n",
      "Epoch 256, Loss: 16034679808.000000\n",
      "Epoch 257, Loss: 420227936.000000\n",
      "Epoch 258, Loss: 4183393792.000000\n",
      "Epoch 259, Loss: 4924185088.000000\n",
      "Epoch 260, Loss: 22017169408.000000\n",
      "Epoch 261, Loss: 40212987904.000000\n",
      "Epoch 262, Loss: 387336032.000000\n",
      "Epoch 263, Loss: 13189605376.000000\n",
      "Epoch 264, Loss: 9291456512.000000\n",
      "Epoch 265, Loss: 1831860480.000000\n",
      "Epoch 266, Loss: 22767720448.000000\n",
      "Epoch 267, Loss: 8582373376.000000\n",
      "Epoch 268, Loss: 4353805312.000000\n",
      "Epoch 269, Loss: 409974464.000000\n",
      "Epoch 270, Loss: 9098360832.000000\n",
      "Epoch 271, Loss: 11418804224.000000\n",
      "Epoch 272, Loss: 15266447360.000000\n",
      "Epoch 273, Loss: 3527149568.000000\n",
      "Epoch 274, Loss: 4409205248.000000\n",
      "Epoch 275, Loss: 7673532416.000000\n",
      "Epoch 276, Loss: 483869408.000000\n",
      "Epoch 277, Loss: 11372343296.000000\n",
      "Epoch 278, Loss: 14535861248.000000\n",
      "Epoch 279, Loss: 4747873280.000000\n",
      "Epoch 280, Loss: 3440576256.000000\n",
      "Epoch 281, Loss: 13484479488.000000\n",
      "Epoch 282, Loss: 8852514816.000000\n",
      "Epoch 283, Loss: 9949950976.000000\n",
      "Epoch 284, Loss: 10883368960.000000\n",
      "Epoch 285, Loss: 20900904960.000000\n",
      "Epoch 286, Loss: 14175101952.000000\n",
      "Epoch 287, Loss: 8838845440.000000\n",
      "Epoch 288, Loss: 4795748864.000000\n",
      "Epoch 289, Loss: 17429602304.000000\n",
      "Epoch 290, Loss: 6095581184.000000\n",
      "Epoch 291, Loss: 3519575296.000000\n",
      "Epoch 292, Loss: 16469171200.000000\n",
      "Epoch 293, Loss: 8902332416.000000\n",
      "Epoch 294, Loss: 308043456.000000\n",
      "Epoch 295, Loss: 10933620736.000000\n",
      "Epoch 296, Loss: 5188523008.000000\n",
      "Epoch 297, Loss: 10913218560.000000\n",
      "Epoch 298, Loss: 13796203520.000000\n",
      "Epoch 299, Loss: 5509381120.000000\n",
      "Epoch 300, Loss: 272274432.000000\n",
      "Epoch 301, Loss: 11103856640.000000\n",
      "Epoch 302, Loss: 12138426368.000000\n",
      "Epoch 303, Loss: 6573504512.000000\n",
      "Epoch 304, Loss: 4044275200.000000\n",
      "Epoch 305, Loss: 5449145344.000000\n",
      "Epoch 306, Loss: 6680047616.000000\n",
      "Epoch 307, Loss: 10214914048.000000\n",
      "Epoch 308, Loss: 495487360.000000\n",
      "Epoch 309, Loss: 6926702080.000000\n",
      "Epoch 310, Loss: 9439070208.000000\n",
      "Epoch 311, Loss: 7155830272.000000\n",
      "Epoch 312, Loss: 9256147968.000000\n",
      "Epoch 313, Loss: 7361717248.000000\n",
      "Epoch 314, Loss: 6570390528.000000\n",
      "Epoch 315, Loss: 256192592.000000\n",
      "Epoch 316, Loss: 14017359872.000000\n",
      "Epoch 317, Loss: 10592914432.000000\n",
      "Epoch 318, Loss: 2941776896.000000\n",
      "Epoch 319, Loss: 9596136448.000000\n",
      "Epoch 320, Loss: 3375001600.000000\n",
      "Epoch 321, Loss: 4592666112.000000\n",
      "Epoch 322, Loss: 4618104320.000000\n",
      "Epoch 323, Loss: 4904146944.000000\n",
      "Epoch 324, Loss: 5829518848.000000\n",
      "Epoch 325, Loss: 2320660224.000000\n",
      "Epoch 326, Loss: 400044096.000000\n",
      "Epoch 327, Loss: 7868889088.000000\n",
      "Epoch 328, Loss: 246338880.000000\n",
      "Epoch 329, Loss: 4400572928.000000\n",
      "Epoch 330, Loss: 5000121856.000000\n",
      "Epoch 331, Loss: 4056620288.000000\n",
      "Epoch 332, Loss: 4537110528.000000\n",
      "Epoch 333, Loss: 2076058368.000000\n",
      "Epoch 334, Loss: 830187072.000000\n",
      "Epoch 335, Loss: 263164448.000000\n",
      "Epoch 336, Loss: 9628505088.000000\n",
      "Epoch 337, Loss: 3624913408.000000\n",
      "Epoch 338, Loss: 1809025408.000000\n",
      "Epoch 339, Loss: 3564293632.000000\n",
      "Epoch 340, Loss: 1638980224.000000\n",
      "Epoch 341, Loss: 3121333760.000000\n",
      "Epoch 342, Loss: 8658513920.000000\n",
      "Epoch 343, Loss: 7028099584.000000\n",
      "Epoch 344, Loss: 6127085568.000000\n",
      "Epoch 345, Loss: 7861168640.000000\n",
      "Epoch 346, Loss: 2419205120.000000\n",
      "Epoch 347, Loss: 3555534080.000000\n",
      "Epoch 348, Loss: 2544499968.000000\n",
      "Epoch 349, Loss: 3019344640.000000\n",
      "Epoch 350, Loss: 2426474752.000000\n",
      "Epoch 351, Loss: 4550135808.000000\n",
      "Epoch 352, Loss: 4245348864.000000\n",
      "Epoch 353, Loss: 1864001920.000000\n",
      "Epoch 354, Loss: 2292824064.000000\n",
      "Epoch 355, Loss: 3342053888.000000\n",
      "Epoch 356, Loss: 240247616.000000\n",
      "Epoch 357, Loss: 3561177856.000000\n",
      "Epoch 358, Loss: 1528807040.000000\n",
      "Epoch 359, Loss: 6163441152.000000\n",
      "Epoch 360, Loss: 1812748672.000000\n",
      "Epoch 361, Loss: 2692778496.000000\n",
      "Epoch 362, Loss: 2624022016.000000\n",
      "Epoch 363, Loss: 2572378112.000000\n",
      "Epoch 364, Loss: 6556790784.000000\n",
      "Epoch 365, Loss: 3337485312.000000\n",
      "Epoch 366, Loss: 1810989056.000000\n",
      "Epoch 367, Loss: 1149714432.000000\n",
      "Epoch 368, Loss: 3446705664.000000\n",
      "Epoch 369, Loss: 2131131776.000000\n",
      "Epoch 370, Loss: 1141661952.000000\n",
      "Epoch 371, Loss: 7085024768.000000\n",
      "Epoch 372, Loss: 180118592.000000\n",
      "Epoch 373, Loss: 4282096384.000000\n",
      "Epoch 374, Loss: 5661916672.000000\n",
      "Epoch 375, Loss: 1898508032.000000\n",
      "Epoch 376, Loss: 2978165504.000000\n",
      "Epoch 377, Loss: 3043557888.000000\n",
      "Epoch 378, Loss: 2218228480.000000\n",
      "Epoch 379, Loss: 4595038208.000000\n",
      "Epoch 380, Loss: 178592672.000000\n",
      "Epoch 381, Loss: 3229625856.000000\n",
      "Epoch 382, Loss: 1653349632.000000\n",
      "Epoch 383, Loss: 1027086272.000000\n",
      "Epoch 384, Loss: 174412608.000000\n",
      "Epoch 385, Loss: 2149094656.000000\n",
      "Epoch 386, Loss: 2350999808.000000\n",
      "Epoch 387, Loss: 5212164608.000000\n",
      "Epoch 388, Loss: 3355991040.000000\n",
      "Epoch 389, Loss: 1764525824.000000\n",
      "Epoch 390, Loss: 941580160.000000\n",
      "Epoch 391, Loss: 821812032.000000\n",
      "Epoch 392, Loss: 3678002432.000000\n",
      "Epoch 393, Loss: 144049952.000000\n",
      "Epoch 394, Loss: 2077197952.000000\n",
      "Epoch 395, Loss: 2774233344.000000\n",
      "Epoch 396, Loss: 2559821568.000000\n",
      "Epoch 397, Loss: 3334500352.000000\n",
      "Epoch 398, Loss: 148000464.000000\n",
      "Epoch 399, Loss: 3694501120.000000\n",
      "Epoch 400, Loss: 815056256.000000\n",
      "Epoch 401, Loss: 4025921024.000000\n",
      "Epoch 402, Loss: 2992192256.000000\n",
      "Epoch 403, Loss: 3001201152.000000\n",
      "Epoch 404, Loss: 1012864000.000000\n",
      "Epoch 405, Loss: 1267739520.000000\n",
      "Epoch 406, Loss: 2206515968.000000\n",
      "Epoch 407, Loss: 2475780608.000000\n",
      "Epoch 408, Loss: 2442554624.000000\n",
      "Epoch 409, Loss: 789255936.000000\n",
      "Epoch 410, Loss: 2019740672.000000\n",
      "Epoch 411, Loss: 4274900992.000000\n",
      "Epoch 412, Loss: 325369408.000000\n",
      "Epoch 413, Loss: 1941520128.000000\n",
      "Epoch 414, Loss: 1568676608.000000\n",
      "Epoch 415, Loss: 1659221120.000000\n",
      "Epoch 416, Loss: 1887243264.000000\n",
      "Epoch 417, Loss: 304171328.000000\n",
      "Epoch 418, Loss: 2099858176.000000\n",
      "Epoch 419, Loss: 1675771008.000000\n",
      "Epoch 420, Loss: 142254320.000000\n",
      "Epoch 421, Loss: 632370432.000000\n",
      "Epoch 422, Loss: 2325244928.000000\n",
      "Epoch 423, Loss: 1995316992.000000\n",
      "Epoch 424, Loss: 807712768.000000\n",
      "Epoch 425, Loss: 626225728.000000\n",
      "Epoch 426, Loss: 169430704.000000\n",
      "Epoch 427, Loss: 2418381056.000000\n",
      "Epoch 428, Loss: 145695328.000000\n",
      "Epoch 429, Loss: 2319816704.000000\n",
      "Epoch 430, Loss: 746308928.000000\n",
      "Epoch 431, Loss: 415625088.000000\n",
      "Epoch 432, Loss: 1170734592.000000\n",
      "Epoch 433, Loss: 1136066560.000000\n",
      "Epoch 434, Loss: 1517959424.000000\n",
      "Epoch 435, Loss: 2481477632.000000\n",
      "Epoch 436, Loss: 922339264.000000\n",
      "Epoch 437, Loss: 1590217472.000000\n",
      "Epoch 438, Loss: 2121921280.000000\n",
      "Epoch 439, Loss: 1202289408.000000\n",
      "Epoch 440, Loss: 1507403904.000000\n",
      "Epoch 441, Loss: 2472818688.000000\n",
      "Epoch 442, Loss: 581489152.000000\n",
      "Epoch 443, Loss: 1199371264.000000\n",
      "Epoch 444, Loss: 1561756672.000000\n",
      "Epoch 445, Loss: 1847322240.000000\n",
      "Epoch 446, Loss: 1821025152.000000\n",
      "Epoch 447, Loss: 2015628032.000000\n",
      "Epoch 448, Loss: 1875642368.000000\n",
      "Epoch 449, Loss: 1435866624.000000\n",
      "Epoch 450, Loss: 765768448.000000\n",
      "Epoch 451, Loss: 1302868992.000000\n",
      "Epoch 452, Loss: 1131267584.000000\n",
      "Epoch 453, Loss: 1952199040.000000\n",
      "Epoch 454, Loss: 3084717056.000000\n",
      "Epoch 455, Loss: 616455680.000000\n",
      "Epoch 456, Loss: 127973248.000000\n",
      "Epoch 457, Loss: 1175127424.000000\n",
      "Epoch 458, Loss: 760461376.000000\n",
      "Epoch 459, Loss: 1229578112.000000\n",
      "Epoch 460, Loss: 1274650112.000000\n",
      "Epoch 461, Loss: 1042130560.000000\n",
      "Epoch 462, Loss: 806022400.000000\n",
      "Epoch 463, Loss: 396745920.000000\n",
      "Epoch 464, Loss: 918613760.000000\n",
      "Epoch 465, Loss: 1272239360.000000\n",
      "Epoch 466, Loss: 1034889216.000000\n",
      "Epoch 467, Loss: 455041376.000000\n",
      "Epoch 468, Loss: 195115248.000000\n",
      "Epoch 469, Loss: 421464096.000000\n",
      "Epoch 470, Loss: 397203744.000000\n",
      "Epoch 471, Loss: 643492480.000000\n",
      "Epoch 472, Loss: 2120850176.000000\n",
      "Epoch 473, Loss: 933859712.000000\n",
      "Epoch 474, Loss: 695078656.000000\n",
      "Epoch 475, Loss: 92048048.000000\n",
      "Epoch 476, Loss: 367388032.000000\n",
      "Epoch 477, Loss: 879085056.000000\n",
      "Epoch 478, Loss: 656385600.000000\n",
      "Epoch 479, Loss: 389141408.000000\n",
      "Epoch 480, Loss: 1210150144.000000\n",
      "Epoch 481, Loss: 469716736.000000\n",
      "Epoch 482, Loss: 810635008.000000\n",
      "Epoch 483, Loss: 389394240.000000\n",
      "Epoch 484, Loss: 106002200.000000\n",
      "Epoch 485, Loss: 1395819904.000000\n",
      "Epoch 486, Loss: 121914160.000000\n",
      "Epoch 487, Loss: 420721632.000000\n",
      "Epoch 488, Loss: 357768000.000000\n",
      "Epoch 489, Loss: 633682560.000000\n",
      "Epoch 490, Loss: 585977280.000000\n",
      "Epoch 491, Loss: 562848896.000000\n",
      "Epoch 492, Loss: 574312320.000000\n",
      "Epoch 493, Loss: 1230571648.000000\n",
      "Epoch 494, Loss: 87506872.000000\n",
      "Epoch 495, Loss: 1153895552.000000\n",
      "Epoch 496, Loss: 82428360.000000\n",
      "Epoch 497, Loss: 1038289792.000000\n",
      "Epoch 498, Loss: 448707936.000000\n",
      "Epoch 499, Loss: 314727264.000000\n",
      "Epoch 500, Loss: 498449312.000000\n",
      "Epoch 501, Loss: 462340736.000000\n",
      "Epoch 502, Loss: 96087856.000000\n",
      "Epoch 503, Loss: 126861792.000000\n",
      "Epoch 504, Loss: 302426656.000000\n",
      "Epoch 505, Loss: 494967872.000000\n",
      "Epoch 506, Loss: 337059840.000000\n",
      "Epoch 507, Loss: 132388856.000000\n",
      "Epoch 508, Loss: 693633920.000000\n",
      "Epoch 509, Loss: 1114562432.000000\n",
      "Epoch 510, Loss: 471275808.000000\n",
      "Epoch 511, Loss: 131432768.000000\n",
      "Epoch 512, Loss: 263412192.000000\n",
      "Epoch 513, Loss: 1279599616.000000\n",
      "Epoch 514, Loss: 457543392.000000\n",
      "Epoch 515, Loss: 458594016.000000\n",
      "Epoch 516, Loss: 622708864.000000\n",
      "Epoch 517, Loss: 498872480.000000\n",
      "Epoch 518, Loss: 137291968.000000\n",
      "Epoch 519, Loss: 758031680.000000\n",
      "Epoch 520, Loss: 915832448.000000\n",
      "Epoch 521, Loss: 678478016.000000\n",
      "Epoch 522, Loss: 378331232.000000\n",
      "Epoch 523, Loss: 762159232.000000\n",
      "Epoch 524, Loss: 705316992.000000\n",
      "Epoch 525, Loss: 650413120.000000\n",
      "Epoch 526, Loss: 84879040.000000\n",
      "Epoch 527, Loss: 82642080.000000\n",
      "Epoch 528, Loss: 316795328.000000\n",
      "Epoch 529, Loss: 477735680.000000\n",
      "Epoch 530, Loss: 375938016.000000\n",
      "Epoch 531, Loss: 281919712.000000\n",
      "Epoch 532, Loss: 239349120.000000\n",
      "Epoch 533, Loss: 615631040.000000\n",
      "Epoch 534, Loss: 255747744.000000\n",
      "Epoch 535, Loss: 303356640.000000\n",
      "Epoch 536, Loss: 203209152.000000\n",
      "Epoch 537, Loss: 918103104.000000\n",
      "Epoch 538, Loss: 217021104.000000\n",
      "Epoch 539, Loss: 212248320.000000\n",
      "Epoch 540, Loss: 126599800.000000\n",
      "Epoch 541, Loss: 518902656.000000\n",
      "Epoch 542, Loss: 194890528.000000\n",
      "Epoch 543, Loss: 235760624.000000\n",
      "Epoch 544, Loss: 190488960.000000\n",
      "Epoch 545, Loss: 102314312.000000\n",
      "Epoch 546, Loss: 659950976.000000\n",
      "Epoch 547, Loss: 871906112.000000\n",
      "Epoch 548, Loss: 315671008.000000\n",
      "Epoch 549, Loss: 556118656.000000\n",
      "Epoch 550, Loss: 799863360.000000\n",
      "Epoch 551, Loss: 753081600.000000\n",
      "Epoch 552, Loss: 391289728.000000\n",
      "Epoch 553, Loss: 474332224.000000\n",
      "Epoch 554, Loss: 190440256.000000\n",
      "Epoch 555, Loss: 193407168.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m complex_mse_loss(output, targets)  \u001b[38;5;66;03m# Custom loss\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tomli\\anaconda3\\envs\\pytorch_311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tomli\\anaconda3\\envs\\pytorch_311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tomli\\anaconda3\\envs\\pytorch_311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the size of our \"measurement\" vector as encoding_dim. This needs to be larger than the sparsity of our matrix\n",
    "\n",
    "encoding_dim = max_sparsity + 1\n",
    "\n",
    "# Initialize model\n",
    "model = LearnedAutoencoder(vector_size, encoding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def complex_mse_loss(input, target):\n",
    "    return F.mse_loss(input.real, target.real) + F.mse_loss(input.imag, target.imag)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch  # Unpack the tuple\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = complex_mse_loss(output, targets)  # Custom loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
